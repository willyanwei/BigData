# SparkSQL  

## 大数据学习线路图

<img src="https://upload-images.jianshu.io/upload_images/22827736-ab17271698b9385a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">

## 概述  
SparkSQL是Spark用来处理结构化数据的一个模块，提供了一个编程抽象叫做DataFrame作为分布式SQL查询引擎的作用外部结构化数据源包括：Parquet默认、JSON、RMDBS、HIVE等。  

## 为什么使用SparkSQL     
HIVESQL-MR: HIVE，将HiveSQL转换成MR然后提交到集群上执行，大大简化了编写MR的程序复杂性    
SparkSQL-SQL-RDD：SparkSQL，将SQL查询转换成SparkCore的应用程序，然后提交到集群执行    
SparkSQL是一个统一的数据处理技术栈，spark整体hive，可以无缝处理hive数据仓库中的数据 


## SparkSQL VS Hive  

<img src="https://upload-images.jianshu.io/upload_images/22827736-0a4d260b243e6616.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">

## 特点  
* 容易整合
* 统一的数据访问格式
* 兼容Hive
* 标准的数据接口  

## 步骤
>     
>     	1. 创建 SparkSession 对象
>     	2. 创建 DataFrame 或 Dataset
>     	3. 在 DataFrame 或 Dataset 之上进行转换和 Action，最重要就是编写 SQL 语句
>     	4. 返回结果（保存结果到 HDFS 中，或直接打印出来）


* 步骤一. 创建SparkSession对象  
	在Spark2.0版本中，引用SparkSession作为DataSet、DataFrame API的切入点，SparkSession封装了SparkConf、SparkContext和SQLContext。为了向后兼容，HiveContext也被保存下来。   
	所以，在SQLContext和HiveContext上可用的API在SparkSession上同样可以使用。  
	SparkSession内部封装了SparkContext，所以计算实际上是由SparkContext完成的。  
	**创建方式**  
		**在spark-shell中创建** 
		SparkSession会被自动初始化一个对象叫做spark，为了向后兼容，Spark-Shell还提供了一个SparkContext初始对象  
		[hadoop@hadoop02 ~]$ ~/apps/spark-2.3.1-bin-hadoop2.7/bin/spark-shell \    
		--master spark://hadoop02:7077 \     
		--executor-memory 512m \     
		--total-executor-cores 1      
			<img src="https://upload-images.jianshu.io/upload_images/22827736-84dde010d0473655.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%"> 

	 **在IDEA中创建**  
	<img src="https://upload-images.jianshu.io/upload_images/22827736-75806afde22609c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">  

* 步骤二. 读取数据源创建DataFrame包括四种方式：

	SparkSQL支持多种数据源，包括常见的JSON,JDBC,Parquet，HDFS，提供了读写各种数据的API
	<img src="https://upload-images.jianshu.io/upload_images/22827736-e76e0339dc733d6d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%">    

	* 读取文本文件创建DataFrame
		1. 在本地创建一个文件，有三列，分别为ID/NAME/AGE，用空格分隔，然后上传到HDFS上： hdfs dfs -put person.txt /  
		2. 在spark shell执行下面命令，读取数据，将每一行的数据使用列分隔符分割  
			先执行 spark-shell --master local[2]  
			val lineRDD= sc.textFile("/person.txt").map(_.split(" "))
		3. 定义case class（相当于表的schema）  
			case class Person(id:Int, name:String, age:Int)  
		4. 将RDD和case class关联  
			val personRDD = lineRDD.map(x => Person(x(0).toInt, x(1), x(2).toInt))  
		5. 将RDD转换成DataFrame  
			val personDF = personRDD.toDF  
		6. 通过SparkSession构建DataFrame  
			使用spark-shell中已经初始化好的SparkSession对象spark生成DataFrame  
			val dataFrame=spark.read.text("/person.txt")

	* 读取Parquet文件创建DataFrame      
			val usersDF = spark.read.load(".../.../.../users.parquet")  
			Parquet格式是SparkSQL的默认数据源,可以通过spark.sql.sources.default文件配置 
	   
	* 读取CSV文件创建DataFrame   
			val usersDF = spark.read.format("csv").load(".../.../.../users.csv")

	* 读取JSON文件创建DataFrame  
			val usersDF = spark.read.format("JSON").load(".../.../.../users.json")
	 	
	**案例：**   
		创建DataFrame  
		数据文件：
		 <img src="https://upload-images.jianshu.io/upload_images/22827736-ed4197fa6959e565.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%" >
		1.在本地创建一个文件，有五列，分别是 id、name、sex、age、department，用逗号分隔，然后上传到 HDFS 上：hdfs dfs -put student.txt /student
		2.在 spark shell 执行下面命令，读取数据，将每一行的数据使用列分隔符分割：  
				val lineRDD = sc.textFile("hdfs://myha01/student/student.txt").map(_.split(","))  
		3.定义 case class（相当于表的 schema）：  
			    case class Student(id:Int, name:String, sex:String, age:Int, department:String)  
		4.将 RDD 和 case class 关联：  
			    val studentRDD = lineRDD.map(x =Student(x(0).toInt, x(1), x(2), x(3).toInt, x(4)))  
		5.将 RDD 转换成 DataFrame：  
			    Spark-2.3 : val studentDF = spark.createDataFrame(studentRDD) 或者 Spark-1.6 : val studentDF = studentRDD.toDF  
		6.对 DataFrame 进行处理：studentDF.show 或者 studentDF.printSchema   
			    <img src="https://upload-images.jianshu.io/upload_images/22827736-107e87a82982ae00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%" >
		

* 步骤三. DataFrame常用操作（在 DataFrame 或 Dataset 之上进行转换和 Action）      

	DF常用操作   
	**在DF、DS上进行transformation、action，编写SQL语句**   
	*  **方法一 DSL风格语法，DSL（domain-specific language）操作DataFrame**   
			DataFrame提供了一个领域特定语言(DSL)以方便操作结构化数据。下面是一些使用示例

		1. 查看DataFrame中的内容，通过调用show方法  
			personDF.show  
		2. 查看DataFrame部分列中的内容  
			查看name字段的数据  
			personDF.select(personDF.col("name")).show  
		3. 打印DataFrame的Schema信息  
			personDF.printSchema    
		4. 查询所有的name和age，并将age+1
			personDF.select(col("id"), col("name"), col("age") + 1).show  
		5. 过滤age大于等于25的，使用filter方法过滤
			personDF.filter(col("age") >= 25).show  
		6. 统计年龄大于30的人数
			personDF.filter(col("age")>30).count()  
		7. 按年龄进行分组并统计相同年龄的人数
			personDF.groupBy("age").count().show   

	**案例：**   
	承接第二步，emp替换了student，是一个意思  
	1.查看所有的员工信息===selec * from empDF;  
	scala>empDF.show    
 	<img src="https://upload-images.jianshu.io/upload_images/22827736-e7b1c3860b0672ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%" >

	2.查询所有的员工姓名 ($符号添加不加功能一样)===select ename,deptno from empDF;  
	scala>empDF.select("ename","deptno").show  
	scala>empDF.select("deptno").show  
	<img src="https://upload-images.jianshu.io/upload_images/22827736-7ad65b2983a54760.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%" >
	3.查询所有的员工姓名和薪水，并给薪水加100块钱===select ename,sal,sal+100 from empDF;  
	scala>empDF.select("sal",$"sal"+100).show  
	<img src="https://upload-images.jianshu.io/upload_images/22827736-440c56b26b3b1334.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%" >
	4.查询工资大于2000的员工===select * from empDF where sal>2000;  
	scala>empDF.filter($"sal" > 2000).show  
	<img src="https://upload-images.jianshu.io/upload_images/22827736-33a85afdbef0ca38.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%" >
	5.分组===select deptno,count(*) from empDF group by deptno;  
	scala>empDF.groupBy("deptno").avg().show  
	scala>empDF.groupBy($"deptno").max().show  
	<img src="https://upload-images.jianshu.io/upload_images/22827736-e245d0a3fa05421b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%" >


	* **方法二 SQL风格语法**    
	DF的强大之处是我们可以将它看做是一个关系型数据库，然后可以通过在程序中使用spark.sql（）来执行SQL查询，结果将作为一个DF返回。  
	如果想使用SQL风格的语法，需要将DataFrame注册成表,采用如下的方式：
	personDF.registerTempTable("t_person")

	1. 查询年龄最大的前两名  
				spark.sql("select * from t_person order by age desc limit 2").show  
	2. 显示表的Schema信息  
				spark.sql("desc t_person").show  
	3. 查询年龄大于30的人的信息  
				spark.sql("select * from t_person where age > 30 ").show  

	**案例**
			
	(1)前提条件：需要把DataFrame注册成是一个Table或者View，四种方法  
	<img src="https://upload-images.jianshu.io/upload_images/22827736-00c1199a3accdf9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%" >  
	例如：  
	scala>empDF.createOrReplaceTempView("emp")  
			
	(2)使用SparkSession执行  
	scala>spark.sql("select * from emp").show    
	scala>spark.sql("select * from emp where deptno=10").show    
	<img src="https://upload-images.jianshu.io/upload_images/22827736-9996c45f9a26b29a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%" > 
	(3)求每个部门的工资总额    
	scala>spark.sql("select deptno,sum(sal) from emp group by deptno").show    
	<img src="https://upload-images.jianshu.io/upload_images/22827736-00fd859732aebf5c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%" >   
	DataFrame 支持的操作  
	<img src="https://upload-images.jianshu.io/upload_images/22827736-add97d10113a35bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%" >   

* 步骤四 保存结果    
（保存结果到 HDFS 中，或直接打印出来）  
通用的load和save功能  

	1. 编写普通的 load 和 save 功能    
spark.read.load("hdfs://myha01/spark/sql/input/users.parquet").select  ("name","favorite_color").write.save("hdfs://myha01/spark/sql/output")    
		
	2. 指定 load 和 save 的特定文件格式    
		spark.read.format("json").load("hdfs://myha01/spark/sql/input/people.json").select("name","age").write.format("csv").save("hdfs://myha01/spark/sql/csv")	    	  

	**案例**    
	usersDF.select($"name")  
	.write   
	.format("csv")    
	.mode("overwrite")    
	.save("/root/tmp/result1")    
	> 存储模式mode  
	> 1. SaveMode.ErrorIfExists--默认存储模式,表已存在则报错    
	> 2. SaveMode.Append  --若表存在，则追加，若不存在，则创建表，再插入数据  
	> 3. SaveMode.Overwrite  --将已有的表和数据删除，重新创建表再插入数据  
	> 4. SaveMode.Ignore  --若表存在，直接跳过数据存储，不存储；若表不存在，创建表，存入数据    
    
   
   
## SparkSQL整合Hive  
* 整合原理：
	SparkSQL操作Hive中的表数据
	Spark可以通过读取hive的元数据来兼容hive，读取hive的表数据，然后在spark引擎中进行sql统计分析，从而，通过sparksql与hive结合实现数据分析将成为一种最佳实践。

* 整合方法：
	拷贝mysql-connector-java-5.1.38-bin.jar等相关的jar包到你${spark_home}/jars中
	cp mysql-connector-java-6.0.4.jar /opt/spark3.1/jars/

	Spark自带的jar放在了spark的jars目录，为了方便控制复制以下jar到spark的jars目录下
	位于$HBASE_HOME/lib目录  
	hbase-annotations-1.1.4.jar
	hbase-client-1.1.4.jar
	hbase-common-1.1.4.jar
	hbase-hadoop2-compat-1.1.4.jar
	hbase-hadoop-compat-1.1.4.jar
	hbase-protocol-1.1.4.jar
	hbase-server-1.1.4.jar
	htrace-core-3.1.0-incubating.jar

	位于$HIVE_HOME/lib目录
	hive-hbase-handler-2.3.2.jar
	
	将hive的hive-site.xml拷贝到放入$SPARK-HOME/conf目录下,里面配置的是Hive metastore元数据存放在数据库的位置
 
* 访问方式：

	* 通过SparkSQL建立连接
	Shell 进入
	root@101Server:/opt/spark3.1/bin# spark-sql \  
	--master spark://server192:7077 \  
	--executor-memory 512m \  
	--total-executor-cores 2 \  


	如果需要查询hive和hbase关联整合的表，需要使用使用hive的库  

	root@101Server:/opt/spark3.1/bin# spark-sql \    
	> --master spark://101Server:7077 \    
	> --executor-memory 512m \    
	> --total-executor-cores 2 \    
	> --driver-class-path /opt/hive3.1/lib/mysql-connector-java-6.0.4.jar \    
	> --jars /opt/hive3.1/lib/hive-hbase-handler-3.1.2.jar    
  
 
	* 通过beeline建立连接(推荐)  
	Beeline是Hive 0.11版本引入的新命令行客户端工具，基于SQLline CLI的JDBC(Java Database Connectivity: Java语言中用来规范客户端程序如何访问数据库的应用程序接口)客户端。     
	在Beeline客户端中，你可以使用标准的HiveQL命令来创建、列举以及查询数据表。Beeline shell的好处是：在多用户间共享的缓存数据表上进行快速的数据探索。   


	Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容HiveServer2。 
	因为 Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用 hive 的 beeline 访问 Spark Thrift Server 执行相关语句。Spark Thrift Server 的目的也只是取代 HiveServer2， 
	因此它依旧可以和 Hive Metastore进行交互，获取到 hive 的元数据。 
	如果想连接 Thrift Server，需要通过以下几个步骤： 

		1. Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下
		2. 把 Mysql 的驱动 copy 到 jars/目录下
		3. 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下
		4. 启动 Thrift Server： sbin/start-thriftserver.sh

 
	使用 beeline 连接 Thrift Server（因为hive也有beeline，建议进入spark的bin目录执行beeline） 

	root@server191:/opt/spark3.1/bin# ./beeline  
	Beeline version 2.3.7 by Apache Hive  
	beeline> !connect jdbc:hive2://192.168.0.192:10000/default  
	Connecting to jdbc:hive2://192.168.0.192:10000/default  
	Enter username for jdbc:hive2://192.168.0.192:10000/default: root  
	Enter password for jdbc:hive2://192.168.0.192:10000/default: ******  
	2022-03-28 11:05:50,542 INFO jdbc.Utils: Supplied authorities: 192.168.0.192:10000  
	2022-03-28 11:05:50,543 INFO jdbc.Utils: Resolved authority: 192.168.0.192:10000  
	Connected to: Apache Hive (version 3.1.2)  
	Driver: Hive JDBC (version 2.3.7)  
	Transaction isolation: TRANSACTION_REPEATABLE_READ  
	0: jdbc:hive2://192.168.0.101:10000/default> show tables;  

	+---------------------+

	|      tab_name       |

	+---------------------+

	| hbase2hive_student  |

	| movie_table         |

	| rating_table        |

	+---------------------+

	3 rows selected (0.162 seconds)

	0: jdbc:hive2://192.168.0.192:10000/default>

 

 

 
