# Spark

## 大数据学习线路图

<img src="https://upload-images.jianshu.io/upload_images/22827736-ab17271698b9385a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">

## Spark起源
Spark是一种快速、通用、可扩展的大数据分析引擎，  
2009年诞生于加州大学伯克利分校AMPLab，  
2010年开源，  
2013年6月成为Apache孵化项目，  
2014年2月成为Apache顶级项目。  
项目是用Scala进行编写。

## 什么是Spark
Spark是基于内存计算的大数据并行计算框架  
扩展了广泛使用的MR计算模型，而且高效的支持更多的计算，包括交互式查询和流处理  

* Spark适用于各种各样原先需要多种不同的分布式平台的场景
	* 批处理--Spark
	* 迭代算法--从某个值开始，不断地由上一步的结果计算（或推断）出下一步的结果
	* 交互式查询--SparkSQL，提供易使用的交互式查询语言，如SQL.DBMS负责执行查询命令，并将查询结果显示在屏幕上
	* 流处理--SparkStreaming  
	通过在一个统一的框架下支持这些不同的计算，可以简单而低耗的把各种处理流程整合在一起。 这样的组合，在实际的数据分析过程中很有意义。  
	Spark的这种特性还大大减轻了原先需要对各种平台分别管理的负担

	> **大一统的软件栈**，各个组件关系密切并且可以相互调用，这种设计有几个好处：  
	> 1、软件栈中所有的程序库和高级组件 都可以从下层的改进中获益。  
	> 2、运行整个软件栈的代价变小了。不需要运 行 5 到 10 套独立的软件系统了，一个机构只需要运行一套软件系统即可。系统的部署、维护、测试、支持等大大缩减。  
	> 3、能够构建出无缝整合不同处理模型的应用。


## Spark使用场景

我们大致把Spark的用例分为两类：数据科学应用和数据处理应用。也就对应的有两种人群：数据科学家和工程师。

* 数据科学任务  
主要是数据分析领域，数据科学家要负责分析数据并建模，具备 SQL、统计、预测建模(机器学习)等方面的经验，以及一定的使用 Python、 Matlab 或 R 语言进行编程的能力。

* 数据处理应用  
工程师定义为使用 Spark 开发 生产环境中的数据处理应用的软件开发者，通过对接Spark的API实现对处理的处理和转换等任务。

## Spark的内置项目
* Spark Core：   
实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。 还包含对弹性分布式数据集（RDD）的API定义
* Spark SQL：   
是 Spark 用来操作结构化数据的程序包。通过 Spark SQL，我们可以使用 SQL 或者 Apache Hive 版本的 SQL 方言(HQL)来查询数据。Spark SQL 支持多种数据源，比 如 Hive 表、Parquet 以及 JSON 等。
* Spark Streaming：  
是 Spark 提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API，并且与 Spark Core 中的 RDD API 高度对应。
* Spark MLlib：   
提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。
* Spark GraphX  
GraphX是Spark面向图计算提供的框架与算法库。GraphX中提出了弹性分布式属性图的概念，并在此基础上实现了图视图与表视图的有机结合与统一；同时针对图数据处理提供了丰富的操作，例如取子图操作subgraph、顶点属性操作mapVertices、边属性操作mapEdges等。GraphX还实现了与Pregel的结合，可以直接使用一些常用图算法，如PageRank、三角形计数等。  
* 集群管理器：   
Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计 算。为了实现这样的要求，同时获得最大灵活性，Spark 支持在各种集群管理器(cluster manager)上运行，包括Hadoop YARN、Apache Mesos，以及 Spark 自带的一个简易调度器，叫作独立调度器。

## 为什么要Spark

* MapReduce模型的痛点:  MR主要适用于批处理  
	* 不擅长实时计算
	MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果。
	* 不擅长流式计算
	流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。
	* 不擅长DAG（有向图）计算  
	DAG：多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。  
	在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。
* Spark适用于各种各样原先需要多种不同的分布式平台的场景
	* 批处理--Spark
	* 迭代算法--从某个值开始，不断地由上一步的结果计算（或推断）出下一步的结果
	* 交互式查询--SparkSQL，提供易使用的交互式查询语言，如SQL.DBMS负责执行查询命令，并将查询结果显示在屏幕上
	* 流处理--SparkStreaming 

## Spark的特点

* 快  
与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的。

* 易用  
Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便的在这些shell中使用Spark集群来验证解决问题的方法

* 通用  
	* Spark提供了统一的解决方案，Spark可以用于：
		* 批处理
		* 交互式查询SparkSQL
		* 实时流处理SparkStreaming
		* 机器学习Spark MLlib
		* 图计算Graphx
	这些不同类型的处理都可以在同一个应用中无缝使用  
	Spark统一的解决方案非常具有吸引力  
	毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物理成本

* 兼容性
	* Spark可以非常方便地与其他的开源产品进行融合。
	* Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。 这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。


## Spark和MR的区别？ 
* Spark并不能完全替代Hadoop,主要用于替代Hadoop中MR的计算模型，借助于YARN实现资源调度管理，借助于HDFS实现分布式存储  
* Spark运算比Hadoop的MR快的原因  


|  | MR | Spark |
| ----| ---- | ---- |
| 计算模式 | 计算都必须转化成Map和Reduce两个操作，但是这个并不适合所有的情况，难以描述复杂的数据处理过程 | Spark的计算模式也属于MR但不局限于Map和Reduce操作。Spark提供的数据集操作类型有很多种，大致分为： Transformations和Actions两大类。   |
| 计算效率 | 磁盘IO开销大。每次执行时都需要从磁盘读取数据，并且在计算完成后需要将中间结果写入磁盘中，IO开销较大 | Spark提供内存计算，中间结果直接放内存中，带来的更高的迭代运算效率 |
| 计算效率 | 一次计算可能需要分解成一系列按顺序执行的MR任务，任务之间的衔接由于涉及到IO开销，会产生较高的延迟。而且，在前一个任务执行完成之前，其他任务无法开始，难以胜任复杂、多阶段的计算任务 | Spark基于DAG的任务调度执行机制，支持DAG图的分布式并行计算的编程框架，减少了迭代过程中数据的落地，挺高了处理的效率，要优于MR的迭代执行机制|


## Spark架构

<img src="https://upload-images.jianshu.io/upload_images/22827736-2b5def0744896a75.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">


* **Driver节点**  
	Spark的驱动器是执行开发程序中的main方法的进程。负责开发人员编写的用来：  
	* 创建SparkContext
	* 创建RDD
	* 进行RDD转化操作和行动操作代码的执行   
	如果使用Spark Shell，当你启动shell时，系统后台会自启动一个Spark驱动器程序，这就是在Spark Shell中加载的一个叫sc的Spark Context对象。如果驱动器程序终止，那么Spark应用也就结束了

	
	>     运行Spark Application的main（）函数，会创建SparkContext。Spark Context负责和Cluster Manager通信，进行资源申请、任务分配和监控等。  
	>     Driver在Spark作业执行时主要负责以下操作：    
	>     把用户程序转为任务： 用户程序application->转换成Task->打包为Stage->打包发送至WorkNode  
	>     跟踪Executor的运行状况
	>     为执行器节点调度任务
	>     UI展示应用运行状况



* Cluster Manager  
负责申请和管理在WorkNode上运行应用所需要的资源，目前包括Spark原生Cluster Manager、Mesos Cluster Manager和 Hadoop YARN Cluster Manager  

* WorkNode  
Executor是Application运行在WorkNode上的一个进程，负责运行Task，并负责将数据存在内存或者磁盘上，每个APP拥有各自独立的Executor。每个Executor则包含一定数量的资源来运行分配给他的任务
	
	> 每个WN上的Executor服务于不同的APP，他们之间是不可以共享数据的。  
	> 与MR计算框架相比，Spark采用Executor具有两大优势：  
	> 1. Executor利用多线程来执行具体的任务，相比较于MR进程模型，使用的资源和启动的开销要小很多  
	> 2. Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，当需要多轮迭代计算的时候，可以将中间结果存储到这个存储模块中，供下次需要时直接使用，而不需要从磁盘中读取，从而有效的减少I/O开销，在交互式查询场景下，可以预先将数据缓存到BlockManager存储模块上，从而提高I/O读写能力
	> 


## 运行流程

* 简述
	1. 构建Spark Application的运行环境，由Driver创建一个Spark Context，分配并监控资源使用情况
	2. Spark Context向资源管理器YARN申请运行Executor资源，并启动Executor进程
	3. SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAGScheduler解析成Stage，然后提交给底层的TaskScheduler处理
	4. Executor向SparkContext申请Task，TaskSheduler将Task发放给Executor运行并提供应用程序代码
	5. Task在Executor运行，并把结果反馈到TaskScheduler，一层一层反馈上去
	6. 运行完最后释放资源

	<img src="https://upload-images.jianshu.io/upload_images/22827736-27880ad39a1a80e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">



* 详述
	* 登录客户端，提交App，例如wordcount
		* Spark Submit
			* 只能提交任务，不能交互
			* spark-submit --master
		* Spark Shell
			* 本地模式
				* 不需要连接到Spark集群上
				* spark-shell
			* 集群模式
				* 需要连接到Spark集群，启动时需要指定Spark Master节点
				* spark-shell --master
	* 启动Driver，创建一个SparkContext对象
		* 以Client模式为例，会在提交的节点上启动一个Driver进程，Driver就是我们的Application（wordcount），  
		由Driver创建一个SparkContext对象，会在内部创建DAGSheduler和TaskScheduler  
		
			* 启动Driver：运行程序的main函数，并且创建SparkContext的进程，这里的应用程序就是我们自己编写并提交给Spark集群的程序		
		
			* 创建SparkContext：SC是Spark功能的主要入口，其代表与Spark集群的连接，能够用来在集群上创建RDD、累加器、广播变量。每个JVM里只能存在一个处于激活状态的SC，在创建新的SC之前必须调用stop()来关闭之前的SC  
				1. 创建DAGScheduler
				DAG是基于用户的transformations操作和Stage阶段划分算法，将一个Spark任务分解成若干个Stage，然后为每个Stage构建一个TaskSet，并交由TaskScheduler（实质上就是逻辑上将Spark任务进行拆分，用户分布式计算）  
				2. 创建TaskScheduler  
				TS是在DS之前创建的，主要用于接收DS分配的TaskSet，通过网络传递给对应的Executor
			

			要创建Spark应用程序之前，需要先创建SC对象，创建SC之前需要构建一个包含有关应用程序信息的SparkConf对象  
			
			SparkConf主要用于配置Spark应用程序，以键值对的方式，对Spark的关键参数做设置，我们在创建SparkConf对象设置的Spark参数优先级会高于Spark环境中的配置信息  
			
			SparkContext在Spark应用中起到Master的作用,主要职能是作为每个Spark应用程序的入口，会告知如何连接到Spark集群，比如local、standalone、yarn、mesos，创建RDD、广播变量到集群  
			
	* 创建Job
		在Driver里的代码，如果遇到actions算子，就会创建一个Job，多个actions对应多个Job  
	* 生成DAG		
		DS会接收Job，会为这个Job生成DAG

		> DAG全称Directed Acyclic Graph，有向无环图。  
		> 一个由顶点和有方向的边构成的图，从任意一个顶点出发，没有任何一条路径会将其带回到出发的顶点  
		
	* 划分Stage
		DAG图中，遇到窄依赖，就将其加入Stage，一旦遇到宽依赖，就另起同一个新的Stage，所以Stage可以是一个或者多个
		
		> DAGScheduler把Job根据宽依赖划分为多个Stage，对划分出来的每个Stage都抽象成一个TaskSet任务集，交给TaskScheduler来进行进一步的调度运行
		
		Stage的划分是以ShuffleDependency宽依赖为依据，也就是说某个RDD的运算需要将数据进行Shuffle时，这个包含了Shuffle依赖关系的RDD将被用作输入信息，构建一个新的Stage,由此为依据划分Stage，可以确保有依赖关系的数据能够按照正确的顺序得到处理和运算  
		
		宽依赖Shuffle Dependency：父RDD的每个分区都可能被多个子RDD分区使用  
		窄依赖Narrow Dependency： 父RDD的每个分区只被某一个子RDD分区使用  

		<img src="https://upload-images.jianshu.io/upload_images/22827736-c28a4c91414c74e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">
		


		**提交Stage**  
		得到一个或多个有依赖关系的Stage,其中直接触发Job的RDD所关联的Stage作为FinalStage生成一个Job实例，这两者的关系进一步存储在resultStageToJob映射表中，用于在该Stage全部完成时做一些后续处理，如报告状态，清理Job相关数据等  
		
		具体提交一个Stage时，首先判断该Stage所依赖的父Stage的结果是否可用：  
		如果父Stage的结果都可用，则提交该Stage  
		如果任何一个父Stage的结果不可用，则迭代尝试提交父Stage  
		> 没有成功提交的Stage会放入到waitingStages列表中 
		> 当一个属于中间过程的Stage任务完成后，DAGScheduler会检查对应Stage的所有任务是否完成，如果都完成了，则DS重新扫描一次waitingStages中的所有Stage，检查他们是否还有任何依赖的Stage没完成，如果没有就可以提交该Stage了   
		
	* 切分Task  
		每个Stage里面Task数目由该Stage最后一个RDD中的Partiton个数决定，一个Partition对应一个task，生成TaskSet
		> TaskScheduler从DAGScheduler的每个Stage接收一组Task，并负责将它们发送到集群上，运行它们   
		
	* TaskScheduler接收TaskSet后调度Task,将task分配给worker节点执行  
		每个Stage的提交，最终是转换成一个TaskSet任务集的提交，DS通过TS接口提交TaskSet，这个TaskSet最终会触发TS构建一个TaskSetManager的实例来管理这个TaskSet的生命周期，对于DS来说，提交Stage的工作到此完成。

		**任务作业完成状态的监控**  
		要保证相互依赖的Job/Stage能够得到顺利的调度执行，DS就必然要监控当前Job/Stage乃至Task的完成情况。  
		
		通过对外（主要是对TS）暴露一系列的回调函数来实现，对于TS来说，这些回调函数主要包括任务的开始结束失败，任务集的失败，DS根据这些Task的生命周期信息进一步维护Job和Stage的状态信息   

		TS可以通过回调函数通知DS具体的Executor的生命状态，如果一个Executor崩溃了，或者由于任何原因与Driver失联，则对应的Stage的ShuffleMapTask的输出结果也将被标记为不可用，这也将导致对应Stage状态的变更，进而影响到相关Job的状态，进一步可能触发对应Stage的重新提交来重新计算获取相关的数据。  
		**任务结果的获取**  
		一个具体的任务在Executor执行完毕后，其结果需要以某种形式返回给DS，根据任务类型的不同，任务的结果返回方式也不同：  
		1. 对于FinalStage所对应的任务，对应类为ResultTask，返回给DS的是运算结果本身；  
		2. 对于ShuffleMapTask，返回给DS的是一个MapStatus对象，MapStatus对象管理了ShuffleMapTask的运算输出结果在BlockManager中的相关存储信息，而非结果本身，这些存储信息将作为下一个Stage任务的获取输入数据的依据。
	
	* Executor执行Task  
		1. Executor通过taskrunner来执行task  
		2. executor接收到的是一个序列化文件，先反序列化/拷贝等，生成Task，Task里有RDD执行算子，一些方法需要的常量。  
		3. 如果接收到很多Task，没接收一个Task，就会从线程池里获取一条线程，用taskrunner来执行task  
		> Task分为两种：  
		> 1. ResultTask： 最后一个Stage对应的Task  
		> 2. ShuffleMapTask：之前所有Stage对应Task
		> Spark程序就是Stage被切分成很多Task，封装到TaskSet里，提交给Executor执行，一个Stage一个Stage执行，每个Task对应一个RDD的partiton，这个task执行就是我们写的算子操作。  
	


	<img src="https://upload-images.jianshu.io/upload_images/22827736-f21e2605d0e77e2b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">
	

* 抽象流程

	* Spark运行过程中会经历三次裂变：  
	在driver代码里，如果遇到actions算子，就会创造一个Job  
	在DAG中，遇到窄依赖，就将其加入Stage,一旦遇到宽依赖，就另起同一个新的Stage  
	在Stage 里面 task 的数目由该 stage 最后一个 RDD 中的 partition 个数决定，一个partition对应一个task    


	<img src="https://upload-images.jianshu.io/upload_images/22827736-cc63a1f5c7fd89ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%">

	<img src="https://upload-images.jianshu.io/upload_images/22827736-11fbb2db5d5cff5c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">
	


## RDD  
* 概念  
	Resilient Distributed Dataset
	弹性分布式数据集  
	是Spark中最基本的数据抽象，代表了一个不可变、可分区、元素可并行计算的集合。    	

	RDD是将数据项拆分成多个分区的集合，存储在集群的工作节点上的内存中，并执行正确的操作  
	* 分布式数据集  
		RDD是只读的，分区记录的集合，每个分区分布在集群的不同节点上  
		RDD并不存储真正的数据，只是对数据和操作的描述
	* 弹性  
		RDD默认存放在内存中，当内存不足时，Spark自动将RDD写入磁盘
	* 容错性
		根据数据血统，可以自动从节点失败中恢复数据

* RDD创建方式
	创建RDD有三种方式：  
	两种初始创建  
	一种继承其他RDD进行创建    
	1 . 集合并行化创建 (通过scala集合创建) scala中的本地集合 -->Spark RDD   
	spark-shell --master spark://192.168.0.191:7077 --total-executor-cores 2 --executor-memory 512m  
	spark-shell --master yarn --deploy-mode client


	scala> val arr = Array(1,2,3,4,5)  
	scala> val rdd = sc.parallelize(arr)  
	scala> val rdd = sc.makeRDD(arr)  
	scala> rdd.collect  	  
	res0: Array[Int] = Array(1, 2, 3, 4, 5)  
	
	<img src="https://upload-images.jianshu.io/upload_images/22827736-7aa124316453edf9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">  

	通过集合并行化方式创建RDD,适用于本地测试,做实验  

	2 外部文件系统 , 比如 HDFS  

	读取HDFS文件系统  
	val rdd2 = sc.textFile("hdfs://hadoop01:9000/words.txt")

	读取本地文件  
	val rdd2 = sc.textFile(“file:///root/words.txt”)  
 
	scala> val rdd2 = sc.textFile("file:////root/word.txt")  
	scala> rdd2.collect  
	res2: Array[String] = Array(hadoop hbase java, hbase java spark, java, hadoop hive hive, hive hbase)  
	
	3 从父RDD转换成新的子RDD  

	调用 Transformation 类的方法，生成新的 RDD  
	只要调用transformation类的算子，都会生成一个新的RDD。RDD中的数据类型，由传入给算子的函数的返回值类型决定  

	注意：action类的算子，不会生成新的 RDD  

	scala> rdd.collect  
	res3: Array[Int] = Array(1, 2, 3, 4, 5)  

	scala> val rdd = sc.parallelize(arr)  
	scala> val rdd2 = rdd.map(_*100)  
	scala> rdd2.collect  
	res4: Array[Int] = Array(100, 200, 300, 400, 500)  		


* RDD操作
 
	* RDD分区  
	说对RDD进行操作 , 实际上是操作的RDD上的每一个分区 , 分区的数量决定了并行的数量 .  
	一个分区运行在一个Worker节点上，但是一个Worker节点可以运行多个分区  

	使用 rdd.partitions.size 查看分区数量  
	scala> rdd.partitions.size  
	res7: Int = 4  
	scala> rdd2.partitions.size  
	res8: Int = 4  
	<img src="https://upload-images.jianshu.io/upload_images/22827736-931dd815cdec8bcd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">  

	* RDD分区个数默认设定规则  
	如果从外部创建RDD,比如从hdfs中读取数据,正常情况下,分区的数量和我们读取的文件的block块数是一致的,但是如果只有一个block块,那么分区数量是2.也就是说最低的分区数量是2  

	如果是集合并行化创建得到的RDD，分区的数量，默认的和最大可用的cores数量相等。（--total-executor-cores > 可用的 cores? 可用的 cores:--total-executor-cores）   


	默认情况下，一个application使用多少个cores，就有多少个分区  
	分区的数量 = 运行任务的可用的cores（默认一个cores，能处理一个任务）  

	* 可以指定分区的数量：  
	通过集合并行化创建的RDD是可以任意修改分区的数量的  
	val rdd = sc.makeRDD(arr,分区的数值)  
  
		scala> val arr = Array(List(1,3),List(4,6))！  
		scala> val rdd3 = sc.parallelize(arr,3)  
		scala> rdd3.partitions.size  
		res1: Int = 3  

	例一：
	假如我们这里使用SparkContext对象的parallelize方法来创建一个RDD，指定分区数为3：
	> val rdd3 = sc.parallelize(Array(1,2,3,4,5,6,7,8),3)  

	那么rdd3 的分区在所有Worker上分布情况可能如下图所示：

	<img src="https://upload-images.jianshu.io/upload_images/22827736-f201ce7c1e7c47f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">   

	例二：读取外部文件RDD的分区  
	正常情况下，读取HDFS中的文件，默认情况下，读到的文件有几个block块，得到的RDD就有几个分区。  

	当读取一个文件，不足一个block块的时候，会是2个分区  
	默认情况下，分区的数量  = 读取的文件的block块的数量，但是至少是2个  
	scala> val rdd1 = sc.textFile("hdfs://hadoop01:9000/hbase-1.2.6-bin.tar.gz")  
	scala> rdd1.partitions.size 
	res2: Int = 1  

	scala> val rdd2 = sc.textFile("hdfs://hadoop01:9000/hadoop-2.8.3.tar.gz")  
	scala> rdd2.partitions.size  
	res3: Int = 1  

	scala> val rdd3 = sc.textFile("hdfs://hadoop01:9000/ideaIU-2017.2.2.exe")  
	scala> rdd3.partitions.size  
	res4: Int = 4  

* RDD的算子  

	RDD的算子，也可以理解成方法或者函数。在Spark中RDD的算子有下面两种：  
	Transformation算子：即转换操作，特点是：延时加载、不会触发计算；  
	Action算子：即执行操作，特点是：立即执行计算  


	RDD中的所有转换都是延时加载，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率的运行。  
	
	**常用的Transformation算子**  
	map(func)：返回一个新的RDD，该RDD由每一个输入元素经过func函数转换而得；  
	filter(func)：返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成；  
	flatMap(func)：类似于map，但是每一个输入元素可以被映射为0个或多个输出元素(所以func函数返回的是一个序列，而不是单个元素)  
	mapPartitions(func)：类似于map，但独立地在RDD的每一个分区上运行，因此在类型为T的RDD上运行时，func函数的类型必须是Iterator[T] => Iterator[U]；  
	mapPartitionsWithIndex(func)：类似于mapPartitions，但是func带有一个整型参数表示分区的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Iterator) => Iterator[U]；  
	sample(withReplacement,fraction,seed)：根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器的种子；  
	union(otherDataset)：对源RDD和参数RDD求并集后返回一个新的RDD；
	intersection(otherDataset)：对源RDD和参数RDD求交集后返回一个新的RDD；  
	distinct([numTasks])：对源RDD进行去重后返回一个新的RDD；   
	groupByKey([numTasks])：在一个(K,V)类型的RDD上调用，返回一个(K,Iterator[V])类型的RDD；  
	reduceByKey(func,[numTasks])：在一个(K,V)类型的RDD上调用，返回一个(K,V)类型的RDD，使用指定的reduce函数，将相同的key的值聚合到一起，与groupByKey类似，reduce的第二个参数可以通过第二个可选参数设置；  
	sortByKey([accending],[numTasks])：在一个(K,V)类型的RDD 上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)类型的RDD；  
	sortBy(func,[accending],[numTasks])：与sortByKey类似，但是更灵活；  
	join(otherDataset,[numTasks])：在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对于的所有元素对在一起的(K,(V,W))类型的RDD；   
	cogroup(otherDataset,[numTasks])：在类型为(K,V)和(K,W)类型的RDD上调用，返回一个(K,(Iterator<V>,Iterator<W>))类型的RDD；  
	cartesian(otherDataset)：求两个RDD的笛卡尔积  
	aggregateByKey(zeroValue)(seqOp,combOp,[numTasks])  
	pipe(command,[envVars])  
	coalesce(numPartitions)  
	repartition(numPartitions)  
	repartitionAndSortWithinPartitions(partitioner)  

	**常用的Action算子**  
	reduce(func)：通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的；  
	collect()：在驱动程序中，以数组的形式返回数据集的所有元素；  
	count()：返回RDD的元素个数；  
	first()：返回RDD的第一个元素，类似于take(1);  
	take(n)：返回一个由数据集的前n个元素组成的数组；  
	takeSample(withReplacement,num,[seed])：返回一个数组，该数组由数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器的种子；  
	saveAsTextFile(path)：将数据集的元素以textFile的形式保存到本地、HDFS、或者其他支持的系统中，对于每个元素，Spark将会调用toString方法，将其转换为文件中的文本；  
	takeOrdered(n,[ordering])：  
	saveAsSequenceFile(path)：将数据集中的元素以Hadoop Sequence的格式保存到指定的目录下，可以使用HDFS或者其他Hadoop支持的文件系统；  
	saveAsObjectFile(path)：  
	countByKey()：针对(K,V)类型的RDD，返回一个(K,Int)类型的Map，表示每一个key对应的元素个数；
	foreach(func)：在数据集的每一个元素上，运行函数func进行更新；  

* RDD 血缘关系  
	RDD 的最重要的特性之一就是血缘关系（Lineage )，它描述了一个 RDD 是如何从父 RDD 计算得来的。如果某个 RDD 丢失了，则可以根据血缘关系，从父 RDD 计算得来。  

	图给出了一个 RDD 执行过程的实例。系统从输入中逻辑上生成了 A 和 C 两个 RDD， 经过一系列转换操作，逻辑上生成了 F 这个 RDD。  

	Spark 记录了 RDD 之间的生成和依赖关系。当 F 进行行动操作时，Spark 才会根据 RDD 的依赖关系生成 DAG，并从起点开始真正的计算。  

	<img src="https://upload-images.jianshu.io/upload_images/22827736-bd32933f90093de9.gif?imageMogr2/auto-orient/strip" width="100%">    

	上述一系列处理称为一个血缘关系（Lineage），即 DAG 拓扑排序的结果。在血缘关系中，下一代的 RDD 依赖于上一代的 RDD。例如，在图中，B 依赖于 A，D 依赖于 C，而 E 依赖于 B 和 D。

* RDD依赖类型  
	根据不同的转换操作，RDD 血缘关系的依赖分为窄依赖和宽依赖。窄依赖是指父 RDD 的每个分区都只被子 RDD 的一个分区所使用。宽依赖是指父 RDD 的每个分区都被多个子 RDD 的分区所依赖。  
	map、filter、union 等操作是窄依赖，  
	groupByKey、reduceByKey 等操作是宽依赖  

	join 操作有两种情况，如果 join 操作中使用的每个 Partition 仅仅和固定个 Partition 进行 join，则该 join 操作是窄依赖，其他情况下的 join 操作是宽依赖。  

	所以可得出一个结论，窄依赖不仅包含一对一的窄依赖，还包含一对固定个数的窄依赖，也就是说，对父 RDD 依赖的 Partition 不会随着 RDD 数据规模的改变而改变。    

	1. 窄依赖  
	1）子 RDD 的每个分区依赖于常数个父分区（即与数据规模无关)。  
	2）输入输出一对一的算子，且结果 RDD 的分区结构不变，如 map、flatMap。  
	3）输入输出一对一的算子，但结果 RDD 的分区结构发生了变化，如 union。  
	4）从输入中选择部分元素的算子，如 filter、distinct、subtract、sample。  
	2. 宽依赖    
	1）子 RDD 的每个分区依赖于所有父 RDD 分区。  
	2）对单个 RDD 基于 Key 进行重组和 reduce，如 groupByKey、reduceByKey。  
	3）对两个 RDD 基于 Key 进行 join 和重组，如 join。    

	<img src="https://upload-images.jianshu.io/upload_images/22827736-c28a4c91414c74e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%"> 

	简单点说：在血缘关系图中，如果一个RDD指向多个下一个RDD，则为宽依赖  

	Spark 的这种依赖关系设计，使其具有了天生的容错性，大大加快了 Spark 的执行速度。RDD 通过血缘关系记住了它是如何从其他 RDD 中演变过来的。当这个 RDD 的部分分区数据丢失时，它可以通过血缘关系获取足够的信息来重新运算和恢复丢失的数据分区，从而带来性能的提升。  

	窄依赖的失败恢复更为高效，它只需要根据父 RDD 分区重新计算丢失的分区即可，而不需要重新计算父 RDD 的所有分区。  
	宽依赖来讲，单个结点失效，即使只是 RDD 的一个分区失效，也需要重新计算父 RDD 的所有分区，开销较大。  

	宽依赖操作就像是将父 RDD 中所有分区的记录进行了“洗牌”，数据被打散，然后在子 RDD 中进行重组。  

* Stage划分  
	Stage划分依据宽依赖  
	
	用户提交的计算任务是一个由 RDD 构成的 DAG，如果 RDD 的转换是宽依赖，那么这个宽依赖转换就将这个 DAG 分为了不同的阶段（Stage)。  
	宽依赖会带来“洗牌”，所以不同的 Stage 是不能并行计算的，后面 Stage 的 RDD 的计算需要等待前面 Stage 的 RDD 的所有分区全部计算完毕以后才能进行。这点就类似于在 MapReduce 中，Reduce 阶段的计算必须等待所有 Map 任务完成后才能开始一样。  
	
	在对 Job 中的所有操作划分 Stage 时，一般会按照倒序进行，即从 Action 开始，遇到窄依赖操作，则划分到同一个执行阶段，遇到宽依赖操作，则划分一个新的执行阶段。后面的 Stage 需要等待所有的前面的 Stage 执行完之后才可以执行，这样 Stage 之间根据依赖关系就构成了一个大粒度的 DAG。  
	
	假设从 HDFS 中读入数据生成 3 个不同的 RDD(A、C 和 E)，通过一系列转换操作后得到新的 RDD(G)，并把结果保存到 HDFS 中。可以看到这幅 DAG 中只有 join 操作是一个宽依赖，Spark 会以此为边界将其前后划分成不同的阶段。  
	
	同时可以注意到，在 Stage2 中，从 map 到 union 都是窄依赖，这两步操作可以形成一个流水线操作，通过 map 操作生成的分区可以不用等待整个 RDD 计算结束，而是继续进行 union 操作，这样大大提高了计算的效率。      

	<img src="https://upload-images.jianshu.io/upload_images/22827736-5009d5332946a53e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%">   

	把一个 DAG 图划分成多个 Stage 以后，每个 Stage 都代表了一组由关联的、相互之间没有宽依赖关系的任务组成的任务集合。在运行的时候，Spark 会把每个任务集合提交给任务调度器进行处理。  

* RDD缓存  
	Spark RDD是惰性求值的，有时候希望能多次使用同一个RDD，如果简单的对RDD调用行动操作，Spark每次都会重新计算RDD以及它的依赖，这样会带来太大的消耗，为了避免多次计算同一个RDD，可以让Spark读数据进行持久化   
	Spark可以使用Transformation算子persist和cache将任意RDD缓存到内存、磁盘文件系统中，这两个算子是Transformations算子，调用这两个算子时不会立即执行缓存，只是做一个要缓存标记，等到后面action算子触发计算时，才会将标记点的数据缓存到指定位置，以供后续的算子快速重用。  
	**持久化级别StorageLevel:**  
	* MEMORY_ONLY
	* MEMORY_ONLY_SER
	* MEMORY_AND_DISK
	* MEMORY_AND_DISK_SER
	* DISK_ONLY  
	
	> 缓存有可能丢失，或者存储于内存中的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。  
	> 通过基于RDD的一系列转换，丢失的数据会被重新计算，由于RDD的各个Parition是相对独立的，因此只需要计算丢失部分即可，不需要重新计算全部Partition的数据。  





## RDD/DataFrame/DataSet  

  

|  | 定义 | 优缺点 |
| ----| ---- | ---- |
| RDD |  Resilient Distributed Dataset 弹性分布式数据集，是Spark中最基本的数据抽象，代表一个不可变、可分区、元素可并行计算的集合  | RDD仅表示数据集，RDD没有元数据，也就是说没有字段语义定义，他需要用户自己优化程序，对程序员要求较高，从不同数据源读取数据相对困难，读取不同的格式数据都必须用户自己定义转换方式合并 |
| DataFrame | DateFrame(表) = RDD（表数据） + Schema（表结构） 可以认为DF就是Spark中的数据表，按列名的方式去组织的一个分布式的数据集RDD，是SparkSQL对结构化数据的抽象  | 与RDD类似，DF也是一个分布式数据容器，DF更像传统数据库的二维表格，除了数据外还记录了数据的结构信息Schema |
| DataSet | 分布式的数据集合，不仅包含数据本身，还记录了数据结构信息（Schema），还包括数据集的类型 | DF是一种特殊的DataSet，DF=DataSet[Row]，即DataSet的子集，相比DataFrame，Dataset提供了编译时类型检查，对于分布式程序来讲，提交一次作业太费劲了（要编译、打包、上传、运行），到提交到集群运行时才发现错误，这会浪费大量的时间，这也是引入Dataset的一个重要原因 |

* 什么是DataFrame  
	由于 RDD 的局限性，Spark 产生了 DataFrame，其中 Schema 是就是元数据，是语义描述信息。
	在 Spark1.3 之前，DataFrame 被称为SchemaRDD。以行为单位构成的分布式数据集合，按照列赋予不同的名称。对 select、fileter、aggregation 和 sort 等操作符的抽象。  
	DataFrame = RDD+Schema = SchemaRDD  

	最简单的理解我们可以认为DataFrame就是Spark中的数据表（类比传统数据库），DataFrame的结构如下：  
	DataFrame（表）= Schema（表结构） + Data（表数据）  
	将文本转换成二维表（在mysql中叫table，在Spark中叫DataFrame），SparkSQL就派上了用场   
	RDD[Row] == DataFrame    
 
	总结：
	DataFrame（表）是Spark SQL对结构化数据的抽象。可以将DataFrame看做RDD。  

	特点： 
	内部数据无类型，统一为 Row    
	DataFrame 是一种特殊类型的 Dataset，DataSet[Row] = DataFrame    
	DataFrame 自带优化器 Catalyst，可以自动优化程序     
	DataFrame 提供了一整套的 Data Source API    



	DataFrame解释  
	A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrameis simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset<Row> to represent a DataFrame.
	
	翻译：  
	DataFrame 是按列名的方式去组织的一个分布式的数据集（RDD），就像关系型数据库里面的一张表，（或者说好比是 R/Python 语言里面的 DataFrame），不过 SparkSQL 这儿的方法比 R/Python 语言里面的 DataFrame 提供的操作方法更丰富，  
	DataFrame 的数据源有如下：结构化的文件，Hive 里面的表，外部的数据库（MySQL 等），已经存在的 RDD。  
	DataFrame 提供了 Scala，Java，Python，R 的编程 API，在 Scala 或者 Java 编程中，一个 DataFrame 表示以行组织的 Rows 的数据集合，在 Scala 的 API 中，DataFrame 就可以看做是 Dataset[Row]的另一种称呼，但是，在 Java 的 API 中，开发者必须使用 Dataset<Row>去表示一个 DataFrame。  
	
	<img src="https://upload-images.jianshu.io/upload_images/22827736-bf5249d9b2e88af1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%"> 
	
	从上图可以看出，DataFrame相比RDD多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化。  




**什么是DataSet**  
DataSet是数据的分布式集合；   
是Spark1.6版本中新添加的一个接口，是DF之上更高一级的抽象  
提供了RDD强类型化，能够使用强大的Iambda函数的优点，以及SparkSQL优化后的执行引擎的优点  

**为什么引入DataSet**  
由于DF的数据类型统一是Row，这是有缺点的：  
Row运行时才进行类型检查，比如salary的类型是字符串，只有语句执行时才进行类型检查；  
所以SparkSQL引入了DataSet，扩展了DF API，提供了编译时类型检查，面向对象风格的API；  

**DataFrame与DataSet的互转**  
DF是一种特殊的DS。   
（1）DataFrame转为 DataSet

df.as[ElementType]这样可以把DataFrame转化为DataSet。

（2）DataSet转为DataFrame 

ds.toDF()这样可以把DataSet转化为DataFrame。

 
**总结：**   
	1. RDD：全称Resilient Distributed Dataset，弹性分布式数据集，Spark中最基础的数据抽象，特点是RDD只包含数据本身，没有数据结构。    
	2. DataFrame：也是一个分布式数据容器，除数据本身，还记录了数据的结构信息，即schema；结构信息便于Spark知道该数据集中包含了哪些列，每一列的类型和数据是什么。  
	3. DataSet：Spark中最上层的数据抽象，不仅包含数据本身，记录了数据的结构信息schema，还包含了数据集的类型，也就是真正把数据集做成了一个java对象的形式，需要先创建一个样例类case class，把数据做成样例类的格式，每一列就是样例类里的属性。  

