# HIVE  


## 大数据学习线路图  
<img src="https://upload-images.jianshu.io/upload_images/22827736-ab17271698b9385a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">
  
  


## 什么是Hive  
1. 由Facebook开源，最初用于解决海量结构化的日志数据统计问题，贡献给了apache基金会  

2. 底层数据存储在HDFS上   
基于Hadoop的一个数据仓库工具，底层仍然是Hadoop  
Hive仅是数据管理的作用，并不会进行数据存储  

3. 通过元数据关联Hive表与HDFS数据
Hive想要管理HDFS数据，就要建立一个关联关系，关联Hive上的表和HDFS上的数据路径  
这个数据依赖一个元数据库记录，一般情况下这个元数据采用关系型数据库，一般使用mysql作为Hive的元数据库（Hive中内置默认的元数据库derby）  

> 元数据  
> Hive中的表和HDFS上数据的映射关系  
> Hive表属性-内部表，外部表，视图  
> 字段信息-字段类型，字段顺序  
> 元数据一旦丢失，整个Hive库和表都会消失   
 

4. 提供类SQL查询功能  
将结构化数据文件映射为一张表数据库表，将结构化数据（每一行上列比较规整的数据）映射为二维表  
将文本中的每一行数据映射为数据库的每一条数据  
将文本中的每一列数据映射为Hive表字段  

5. 将SQL转化成MR任务的工具  
其本质是将SQL转换成MR的任务进行计算，甚至可以说Hive是MR的一个客户端  
Hive库中存储了很多MR模板，当用户执行一条SQL时，会被翻译成MR任务执行  

6. 使不熟悉MR的用户也能很方便的使用HSQL处理和计算HDFS上的结构化数据
7. 适用离线的批量处理  

## 为什么需要Hive 


1. HDFS解决分布式存储问题，MR解决分布式计算问题
但是学习过程很痛苦，需要一定的编程基础-JAVA,Linux，MR编程，MR套路和原理  
在Hadoop编程过程中，在解决计算问题的时候，基本都是针对结构化数据，事实上在实际生产过程中接触到的数据比如日志数据，是比较规整的结构化数据，针对结构化数据最好的分析方式是SQL  
比如JION操作，实现两表的关联
如果使用MR编程，至少要写60行代码
如果使用SQL只需一行：
select * from a join b on a.pid = b.pid 


## Hive的优缺点  
* 优点  
	* 可扩展性强  
	Hive可以自由的扩展集群的规模，一般情况下不需要重启服务  
	横向扩展：通过分担压力的方式扩展集群的规模  
	纵向扩展：服务器配置升级，CPU内存等  
	* 延展性  
	支持自定义函数，可以根据自己的需求来实现自己的函数  
	* 容错性  
	可以保证及时有节点出现问题，SQL语句仍可完成执行  

* 缺点  
	* 不支持记录级别的增删改操作  
	但是用户可以通过查询生成的新表或者将查询结果导入到文件中  
	* 延迟大  
	MR的启动过程消耗很长时间，所以不能用在交互查询系统中  
	* 不支持事务  
	因为没有增删改，所以只要用来做OLAP而不是OLTP

> OLTP? OLAP?  
> OLTP（on-line transaction processing）翻译为联机事务处理，   
> OLAP（On-Line Analytical Processing）翻译为联机分析处理，  
> 从字面上来看OLTP是做事务处理，OLAP是做分析处理。  
> 从对数据库操作来看，OLTP主要是对数据的增删改，OLAP是对数据的查询。  


## Hive架构  
* 用户接口层
	* CLI-Command line interface  shell命令行
	* JDBC/ODBC是Hive的JAVA实现，有传统的JDBC类似
	* WebGUI 通过浏览器访问Hive
* 元数据存储  
	* 通常存储在关系型数据库如mysql/derby中  
	* Hive中的元数据包括表的名字，表的列，分区及其属性（是否为外部表），表的数据所在目录
* Thrift服务器  
	* 用来提供一个跨语言的服务，Java等各种语言操作Hive  
* 驱动层  
	完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划生成。
	生成的查询计划存储在HDFS中，并随后由MR调用执行  
	* 编译器  
	驱动整个HQL的运行，HQL语句解析为MR程序，最终将MR程序提交到Hadoop  
	* 优化器  
	在编译过程中肯定会有很多SQL转化成MR程序，存在包含关系、重复的操作，在优化器中可以将重复的操作过滤掉  
	* 执行器  
	将SQL语句通过Hive自带的MR模板，编译成MR程序，首先生成一个逻辑执行计划，再生成一个物理执行计划  

<img src="https://upload-images.jianshu.io/upload_images/22827736-c7ea6a9f54847c96.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">


<img src="https://upload-images.jianshu.io/upload_images/22827736-9b9dbf616afb017a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="70%">



## Hive的数据组成  
* 库  DataBase  
* 表
	* 内部表
	* 外部表
		* 内部表和外部表是两个相对的概念，区别在于删除表时是否删除数据  
		* 内部表删除表时会删除数据和元数据   
		* 外部表删除表时只会删除元数据  
		* 一般存储公共数据的表存放为外部表

	* 分区表  
		* Hive存储海量数据，海量的数据查询一定要注意避免全表扫描
		* 查询时为了提升查询性能，设计了分区表，将数据按照用户的业务存储到不同的目录下
		* 一般用日期作为分段字段  
		> 北京  天津  石家庄  深圳  
		> 假设我们存储在一个普通表City中，每次查询都需要进行全表扫描，性能不高  
		> 普通表：
		> /Hive/warehouse/log.txt
		> 分区表：存储的指定不同目录就是分区字段  
		> /Hive/warehouse/city=beijing/log_beijing.txt 
		> /Hive/warehouse/city=tianjin/log_tianjin.txt 
		> /Hive/warehouse/city=shijiazhuang/log_shijiazhuang.txt 
		> /Hive/warehouse/city=shenzhen/shenzhen.txt 
		> 在执行数据查询时只会对指定分区进行查询
		> select * from stu where city = beijing
	* 分桶表
		类似于Hadoop中的分区，程序决定的，只能指定桶的个数（分区的个数）
		根据hash算法，将余数不同的输出到不同的文件中  
		> /Hive/warehouse/part-r-00000
		> /Hive/warehouse/part-r-00001
		> /Hive/warehouse/part-r-00002
		
		作用：  
		提升join的性能  
		提升数据样本的抽取效率，直接拿桶中的数据作为样本数据  
* 视图  
	* 在Hive中仅仅存在逻辑视图不存在物理视图
	* 物理视图：将SQL语句的执行结果存储在视频中
	create view my_view as select id, name, sex from stu; -------别名
	select * from my_view;  -------视图执行 
* 数据存储  
	* 原始数据存储在HDFS上
	* 元数据存储在MySQL中

## Hive的登录方式

* bin/hive

* 使用SQL语句或者SQL脚本进行交互

* 把Hive进程挂载到后台或者前台（推荐）

* 客户端Dbeaver

## Hive的基本操作  
* 数据库操作
* 数据表操作
	* 外部表
	* 内部表
	* 分区表
	* 分桶表


## 案例

* Wordcount案例

	* 建表  
create table docs(line string);  
	* 加载数据到表里  
load data local inpath '/hadoop/hadoop2.9/word.txt' into table docs;  
load data inpath 'hdfs://test/word.txt' into table docs  
	* 按照空格切割查询,形成数组  
select split(line,' ') from docs;  
执行  
hive> select split(line,' ') from docs;  
OK  
["from","cell_monitor","cm","",""]  
["insert","overwrite","table","cell_drop_monitor"]  
[""]  
["from","cell_monitor","cm","",""]  
["insert",""]  

	* explode(array) 数组一条记录有多个参数,将参数拆分,每个参数生成一列  
hive> select explode(split(line,' '))from docs;  
OK  
from  
cell_monitor  
cm  


		insert  
overwrite  
table  
cell_drop_monitor  

		from  
cell_monitor  
cm  

	* 创建结果表  
create table wc(word string,totalword int);  

	* 统计SQL语句  
from (select explode(split(line,' ')) as word from docs) w insert into table wc    
select word, count(1) as totalword  
group by word  
order by word;  

	* 结果  
hive> select * from wc;  
OK  
cell_drop_monitor	1  
cell_monitor	2  
cm	2  
from	2  
insert	2  
overwrite	1  
table	1  
Time taken: 0.18 seconds, Fetched: 8 row(s)  
